{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = \"\"\"\n",
    "For Pretrained Language Models (PLMs), their susceptibility to noise has recently been linked to subword segmentation. However, it is unclear which aspects of segmentation affect their understanding. This study assesses the robustness of PLMs against various disrupted segmentation caused by noise. An evaluation framework for subword segmentation, named Contrastive Lexical Semantic (CoLeS) probe, is proposed. It provides a systematic categorization of segmentation corruption under noise and evaluation protocols by generating contrastive datasets with canonical-noisy word pairs. \n",
    "Experimental results indicate that PLMs are unable to accurately compute word meanings if the noise introduces completely different subwords, small subword fragments, or a large number of additional subwords, particularly when they are inserted within other subwords.\n",
    "These findings provide insight into the PLMs' performance improvement ( guiding future work to design methods to resolve specific corruption types one by one, which relieves the challenges of resolving them all at once) and protection from potential attacks that exploit these issues.\n",
    "\"\"\"\n",
    "bert_abstract = \"\"\"\n",
    "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n",
    "BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n",
    "\"\"\"\n",
    "\n",
    "shortcut_abstract = \"\"\"\n",
    "Recent studies report that many machine reading comprehension (MRC) models can perform closely to or even better than humans on benchmark datasets. However, existing works indicate that many MRC models may learn shortcuts to outwit these benchmarks, but the performance is unsatisfactory in real-world applications. In this work, we attempt to explore, instead of the expected comprehension skills, why these models learn the shortcuts. Based on the observation that a large portion of questions in current datasets have shortcut solutions, we argue that larger proportion of shortcut questions in training data make models rely on shortcut tricks excessively. To investigate this hypothesis, we carefully design two synthetic datasets with annotations that indicate whether a question can be answered using shortcut solutions. We further propose two new methods to quantitatively analyze the learning difficulty regarding shortcut and challenging questions, and revealing the inherent learning mechanism behind the different performance between the two kinds of questions. A thorough empirical analysis shows that MRC models tend to learn shortcut questions earlier than challenging questions, and the high proportions of shortcut questions in training sets hinder models from exploring the sophisticated reasoning skills in the later stage of training.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['lay_summary', 'article', 'headings', 'keywords', 'id'])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BART and PEGASUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTRLSUM - Keyword Summarization and prompt summarization (no prompt in decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, PreTrainedTokenizerFast\n",
    "from transformers.models.bart.modeling_bart import BartForConditionalGeneration\n",
    "\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"hyunwoongko/ctrlsum-cnndm\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"hyunwoongko/ctrlsum-arxiv\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"hyunwoongko/ctrlsum-bigpatent\")\n",
    "\n",
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained(\"hyunwoongko/ctrlsum-cnndm\")\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"hyunwoongko/ctrlsum-arxiv\")\n",
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained(\"hyunwoongko/ctrlsum-bigpatent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(prompt, abstract, include_decoder_input=False):\n",
    "    print(\"Keyword/Prompt \\n \\t \", prompt)\n",
    "    data = tokenizer(abstract+prompt, return_tensors=\"pt\")\n",
    "    input_ids, attention_mask = data[\"input_ids\"], data[\"attention_mask\"]\n",
    "    if include_decoder_input:\n",
    "        summary =  tokenizer.batch_decode(model.generate(input_ids, attention_mask=attention_mask, num_beams=5, decoder_input_ids=tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"][:, :-1]))[0] \n",
    "    else:\n",
    "        summary =  tokenizer.batch_decode(model.generate(input_ids, attention_mask=attention_mask, num_beams=5))[0]\n",
    "        \n",
    "    print(\"Summary \\n \\t \", summary)\n",
    "    print('\\n')\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword/Prompt \n",
      " \t  Q:What is the the research question or questions in the paper? A: => \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinzhe/miniconda3/envs/baseline/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary \n",
      " \t  </s> in this paper, we investigate the impact of subword segmentation corruption on the robustness</s>\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  Q:What is the the proposed method in the paper? A: => \n",
      "Summary \n",
      " \t  </s> in this paper, we propose a novel evaluation framework for subword segmentation, named contrast</s>\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  Q:What are the findings? A: => \n",
      "Summary \n",
      " \t  </s> in this paper, we investigate the impact of subword segmentation corruption on the robustness</s>\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  findings => \n",
      "Summary \n",
      " \t  </s> in this paper, we investigate the impact of subword segmentation corruption on the robustness</s>\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  What are the findings? => \n",
      "Summary \n",
      " \t  </s> in this paper, we investigate the impact of subword segmentation corruption on the robustness</s>\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  the main contributions of this paper are: (1) => \n",
      "Summary \n",
      " \t  </s> in this paper, we investigate the impact of subword segmentation corruption on the performance of</s>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prompt\n",
    "_ = summarize(\"Q:What is the the research question or questions in the paper? A: => \", abstract)\n",
    "# prompt+abtract: </s> in this paper, we propose a novel evaluation framework for subword segmentation, named the</s>\n",
    "# abtract+prompt: </s> in this paper, we investigate the impact of subword segmentation corruption on the robustness</s>\n",
    "\n",
    "_ = summarize(\"Q:What is the the proposed method in the paper? A: => \", abstract)\n",
    "#  prompt+abtract: </s> in this paper, we propose an evaluation framework for subword segmentation, named contrastive</s>\n",
    "#  abtract+prompt: </s> in this paper, we propose a novel evaluation framework for subword segmentation, named contrast</s>\n",
    "\n",
    "_ = summarize(\"Q:What are the findings? A: => \", abstract  )\n",
    "#  prompt+abtract: </s> this paper presents an evaluation framework for subword segmentation based on the concept of contrastive</s>\n",
    "#  abtract+prompt: </s> in this paper, we investigate the impact of subword segmentation corruption on the robustness</s>\n",
    "\n",
    "# keyword\n",
    "_ = summarize(\"findings => \", abstract )\n",
    "#  prompt+abtract: </s> in this paper, we investigate the impact of noise on the robustness of word segmentation</s>\n",
    "#  abstract+prompt: </s> in this paper, we investigate the impact of subword segmentation corruption on the robustness</s>\n",
    "\n",
    "_ = summarize(\"What are the findings? => \", abstract  ) # work like a language model to complete the question\n",
    "#  prompt+abtract: </s> what are the findings of this study on the robustness of word segmentation under noise?</s>\n",
    "#  abstract+prompt: </s> in this paper, we investigate the impact of subword segmentation corruption on the robustness</s>\n",
    "\n",
    "# _ = summarize(\"What are the findings of this study on the robustness of word segmentation under noise? => \", abstract  )\n",
    "\n",
    "_ = summarize(\"the main contributions of this paper are: (1) => \", abstract )\n",
    "#  prompt+abtract: </s> in this paper, we propose an evaluation framework for subword segmentation, named contrastive</s>\n",
    "#  prompt+abtract: </s> in this paper, we investigate the impact of subword segmentation corruption on the performance of</s>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword/Prompt \n",
      " \t  What are the findings of this study on the robustness of word segmentation under noise? => \n",
      "Summary \n",
      " \t  </s> what are the findings of this study on the robustness of word segmentation under noise?</s>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prompt\n",
    "# _ = summarize(\"Q:What is the the research question or questions in the paper? A: => \", bert_abstract)\n",
    "# </s> in this paper, we introduce a new language representation model called bidirectional encoder and</s>\n",
    "\n",
    "# _ = summarize(\"Q:What is the the proposed method in the paper? A: => \", bert_abstract)\n",
    "# </s> in this paper, we introduce a new language representation model called the bidirectional encoder</s>\n",
    "\n",
    "# _ = summarize(\"Q:What are the findings? A: => \", bert_abstract  )\n",
    "# </s> in this paper, we introduce a new language representation model called the bidirectional encoder</s>\n",
    "\n",
    "\n",
    "\n",
    "# keyword\n",
    "# _ = summarize(\"findings => \", bert_abstract )\n",
    "# </s> in this paper, we introduce a new language representation model called the bidirectional encoder</s>\n",
    "\n",
    "# _ = summarize(\"the main contributions of this paper are: (1) => \", bert_abstract )\n",
    "# </s> in this paper, we introduce a new language representation model called the bidirectional encoder</s>\n",
    "\n",
    "# _ = summarize(\"What are the findings? => \", bert_abstract  ) # unlike previous one which works like a language model to complete the question\n",
    "# </s> in this paper, we introduce a new language representation model called the bidirectional encoder</s>\n",
    "\n",
    "# _ = summarize(\"What are the findings of this study on the robustness of word segmentation under noise? => \", bert_abstract  )\n",
    "# </s> what are the findings of this study on the robustness of word segmentation under noise?</s>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTRLSUM - Different decoding algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from allennlp_models.generation.models import Bart\n",
    "from allennlp.data import Vocabulary\n",
    "import torch\n",
    "from allennlp.nn.beam_search import BeamSearch\n",
    "\n",
    "def summary_by_allennlp( prompt, context ):\n",
    "    print(\"Keyword/Prompt \\n \\t \", prompt)\n",
    "    vocab = Vocabulary.from_pretrained_transformer(\"hyunwoongko/ctrlsum-arxiv\") \n",
    "    bart_model = Bart(model_name=\"hyunwoongko/ctrlsum-arxiv\", vocab=vocab)\n",
    "    data = tokenizer([context+prompt], return_tensors=\"pt\", padding=\"longest\" )\n",
    "    input_ids, attention_mask = data[\"input_ids\"], data[\"attention_mask\"]\n",
    "    beam_search = BeamSearch(bart_model._end_id, vocab=vocab, beam_size=5)\n",
    "    initial_decoder_id = torch.tensor( [[bart_model._decoder_start_id]], dtype=input_ids.dtype, device=input_ids.device ).repeat(input_ids.shape[0], 1)\n",
    "    initial_state = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"input_mask\": attention_mask\n",
    "    }\n",
    "\n",
    "    beam_result = beam_search.search(initial_decoder_id, initial_state, bart_model.take_step)\n",
    "    predictions = beam_result[0] # (bsz, beam_size, seq_len)\n",
    "\n",
    "    max_pred_indices = (\n",
    "        beam_result[1].argmax(dim=-1).view(-1, 1, 1).expand(-1, -1, predictions.shape[-1])\n",
    "    ) # (bsz, 1, seq_len)\n",
    "    out = predictions.gather(dim=1, index=max_pred_indices)# (bsz, 1, seq_len)\n",
    "    out = out.squeeze(dim=1) \n",
    "\n",
    "    predicted_text = bart_model.make_output_human_readable({\"predictions\": out})['predicted_text']\n",
    "\n",
    "    print(\"Summary \\n \\t \", predicted_text[0])\n",
    "    print('\\n')\n",
    "    return predicted_text[0]\n",
    "\n",
    "    # import numpy as np\n",
    "    # print(max_pred_indices.numpy().shape)\n",
    "    # print(predictions.numpy().shape)\n",
    "    # np.take(predictions.numpy(), indices=max_pred_indices.numpy(), axis=1) # (2, 2, 1, 50, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword/Prompt \n",
      " \t  Q:What is the the research question or questions in the paper? A: => \n",
      "Summary \n",
      " \t   in this paper, we introduce a new language representation model called the bidirectional encoder and discriminator ( bidirectional encoder and discriminator ) ( darc ) model called the bidirectional encoder and discriminator ( bidirectional\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  Q:What is the the proposed method in the paper? A: => \n",
      "Summary \n",
      " \t   in this paper, we introduce a new language representation model called the bidirectional encoder and discriminator ( bidirectional encoder and discriminator ) ( darc ) model called the bidirectional encoder and discriminator ( bidirectional\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  Q:What are the findings? A: => \n",
      "Summary \n",
      " \t   in this paper, we introduce a new language representation model called the bidirectional encoder and discriminator ( bidirectional encoder and discriminator ) ( darc ) model called the bidirectional encoder and discriminator ( bidirectional\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  findings => \n",
      "Summary \n",
      " \t   in this paper, we introduce a new language representation model called _ bidirectional symbolic encoder and symbolic encoder _ ( bidirectional sceoder and symbolic encoder ), which stands for bidirectional symbolic encoder and symbolic encoder\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  the main contributions of this paper are: (1) => \n",
      "Summary \n",
      " \t   in this paper, we introduce a new language representation model called the bidirectional encoder and discriminator ( bidirectional encoder and discriminator ) ( darc ), which stands for bidirectional encoder and discriminator ( bidirection\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prompt\n",
    "_ = summary_by_allennlp(\"Q:What is the the research question or questions in the paper? A: => \", bert_abstract)\n",
    "# prompt+abstract: </s> in this paper, we introduce a new language representation model called bidirectional encoder and</s>\n",
    "# prompt+abstract allennlp: in this paper, we introduce a new language representation model, called bidirectional language representation model called ( bidirectional language representation model ) called ( bidirectional language representation model ), which is designed to pre- train deep bidirectional representations\n",
    "# abstract+prompt allenlp: in this paper, we introduce a new language representation model called the bidirectional encoder and discriminator ( bidirectional encoder and discriminator ) ( darc ) model called the bidirectional encoder and discriminator ( bidirectional\n",
    "\n",
    "\n",
    "_ = summary_by_allennlp(\"Q:What is the the proposed method in the paper? A: => \", bert_abstract)\n",
    "# prompt+abstract: </s> in this paper, we introduce a new language representation model called the bidirectional encoder</s>\n",
    "# prompt+abstract allennlp:  in this paper, we introduce a new language representation model called the bidirectional language representation model ( dla ), which is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context\n",
    "# abstract+prompt allennlp:  in this paper, we introduce a new language representation model called the bidirectional encoder and discriminator ( bidirectional encoder and discriminator ) ( darc ) model called the bidirectional encoder and discriminator ( bidirectional\n",
    "\n",
    "_ = summary_by_allennlp(\"Q:What are the findings? A: => \", bert_abstract  )\n",
    "# prompt+abstract: </s> in this paper, we introduce a new language representation model called the bidirectional encoder</s>\n",
    "# abstract+prompt allennlp:  in this paper, we introduce a new language representation model called the bidirectional encoder and discriminator ( bidirectional encoder and discriminator ) ( darc ) model called the bidirectional encoder and discriminator ( bidirectional\n",
    "\n",
    "# keyword\n",
    "_ = summary_by_allennlp(\"findings => \", bert_abstract )\n",
    "# prompt+abstract: </s> in this paper, we introduce a new language representation model called the bidirectional encoder</s>\n",
    "# abstract+prompt allennlp: in this paper, we introduce a new language representation model called _ bidirectional symbolic encoder and symbolic encoder _ ( bidirectional sceoder and symbolic encoder ), which stands for bidirectional symbolic encoder and symbolic encoder\n",
    "\n",
    "_ = summary_by_allennlp(\"the main contributions of this paper are: (1) => \", bert_abstract )\n",
    "# prompt+abstract: </s> in this paper, we introduce a new language representation model called the bidirectional encoder</s>\n",
    "# abstract+prompt allennlp: in this paper, we introduce a new language representation model called the bidirectional encoder and discriminator ( bidirectional encoder and discriminator ) ( darc ), which stands for bidirectional encoder and discriminator ( bidirection\n",
    "\n",
    "# _ = summary_by_allennlp(\"What are the findings? => \", bert_abstract  ) # unlike previous one which works like a language model to complete the question\n",
    "# prompt+abstract: </s> in this paper, we introduce a new language representation model called the bidirectional encoder</s>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword/Prompt \n",
      " \t  Q:What is the the research question or questions in the paper? A: => \n",
      "Summary \n",
      " \t   in this paper, we propose a novel evaluation framework for subword segmentation under noise, named the _ contrastive comparative subword segmentation probe _, named the _ contrastive comparative subword segmentation probe _, to be applied to real -\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  Q:What is the the proposed method in the paper? A: => \n",
      "Summary \n",
      " \t   in this paper, we propose an evaluation framework for subword segmentation, named contrastive lexical semantic probe, to assess the robustness of subword segmentation against various disrupted segmentation caused by noise. the proposed framework is applied to the\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  Q:What are the findings? A: => \n",
      "Summary \n",
      " \t   this paper presents an evaluation framework for subword segmentation, named contrastive lexical semantic probe, to assess the robustness of subword segmentation against various disrupted segmentation caused by noise. the framework is applied to the problem of segmentation\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  findings => \n",
      "Summary \n",
      " \t   in this paper, we investigate the impact of noise on the robustness of word segmentation in the context of a novel segmentation evaluation framework, named _ contrastive lexical semantic probe_. our results indicate that the robustness of word segmentation\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  What are the findings? => \n",
      "Summary \n",
      " \t   what are the findings of this study on the robustness of word segmentation under noise, and how can they be used to improve the performance of word segmentation in the presence of noise? + + _ keywords _ : word segmentation, noise,\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  the main contributions of this paper are: (1) => \n",
      "Summary \n",
      " \t   in this paper, we propose a novel evaluation framework for subword segmentation, named contrastive lexical - strategic - narrow - search ( contrastive lexical - strategic - narrow - search ) probe, to assess the robustness of subword segment\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  What are the findings of this study on the robustness of word segmentation under noise? => \n",
      "Summary \n",
      " \t   this study assesses the robustness of word segmentation against various disrupted segmentation caused by noise by generating contrastive datasets with canonical-noisy word pairs. results indicate that the robustness of word segmentation under noise depends on various disrupted segmentation\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prompt\n",
    "_ = summary_by_allennlp(\"Q:What is the the research question or questions in the paper? A: => \", abstract)\n",
    "# prompt+abstract: </s> in this paper, we propose a novel evaluation framework for subword segmentation, named the</s>\n",
    "# prompt+abstract allennlp: in this paper, we propose a novel evaluation framework for subword segmentation under noise, named the _ contrastive comparative subword segmentation probe _, named the _ contrastive comparative subword segmentation probe _, to be applied to real -\n",
    "\n",
    "_ = summary_by_allennlp(\"Q:What is the the proposed method in the paper? A: => \", abstract)\n",
    "# prompt+abstract: </s> in this paper, we propose an evaluation framework for subword segmentation, named contrastive</s>\n",
    "\n",
    "_ = summary_by_allennlp(\"Q:What are the findings? A: => \", abstract  )\n",
    "# prompt+abstract: </s> this paper presents an evaluation framework for subword segmentation based on the concept of contrastive</s>\n",
    "\n",
    "\n",
    "\n",
    "# keyword\n",
    "_ = summary_by_allennlp(\"findings => \", abstract )\n",
    "# prompt+abstract: </s> in this paper, we investigate the impact of noise on the robustness of word segmentation</s>\n",
    "\n",
    "_ = summary_by_allennlp(\"What are the findings? => \", abstract  ) # work like a language model to complete the question\n",
    "# prompt+abstract: </s> what are the findings of this study on the robustness of word segmentation under noise?</s>\n",
    "\n",
    "_ = summary_by_allennlp(\"the main contributions of this paper are: (1) => \", abstract )\n",
    "# prompt+abstract: </s> in this paper, we propose an evaluation framework for subword segmentation, named contrastive</s>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTRLSUM - Prompt summarization (prompt in decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword/Prompt \n",
      " \t  Q:What is the the proposed method in the paper? A: => \n",
      "Summary \n",
      " \t  <s>Q:What is the the proposed method in the paper? A: =>  scrutinizing</s>\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  Q:What are the findings? => \n",
      "Summary \n",
      " \t  <s>Q:What are the findings? => :\\ { # 1 } # 1 # 1</s>\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  the main contributions of this paper are: (1) => \n",
      "Summary \n",
      " \t  <s>the main contributions of this paper are: (1) => ength - based segmentation</s>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# put keywords into decoder would generate nonsense\n",
    "# _ = summarize(\"findings => \", abstract , include_decoder_input=True ) \n",
    "\n",
    "# QA prompt\n",
    "_ = summarize(\"Q:What is the the proposed method in the paper? A: => \", abstract , include_decoder_input=True)\n",
    "_ = summarize(\"Q:What are the findings? A: => \", abstract , include_decoder_input=True )\n",
    "\n",
    "# Contribution prompt\n",
    "_ = summarize(\"the main contributions of this paper are: (1) => \", abstract, include_decoder_input=True )\n",
    "# _ = summarize(\"The main contribution of this paper are: (1) => \", abstract, include_decoder_input=True ) # whether the model is sensitive to the little change of prompts?\n",
    "\n",
    "\n",
    "# Input length of decoder_input_ids is 21, but ``max_length`` is set to 20.\n",
    "# _ = summarize(\"What are the findings of this study on the robustness of word segmentation under noise? => \", abstract, include_decoder_input=True )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt \n",
      " \t  Q:What is the the proposed method in the paper? A: => \n",
      "Summary \n",
      " \t  </s> in this paper, we introduce a new language representation model called the bidirectional encoder</s>\n",
      "\n",
      "\n",
      "Prompt \n",
      " \t  findings => \n",
      "Summary \n",
      " \t  </s> in this paper, we introduce a new language representation model called the bidirectional encoder</s>\n",
      "\n",
      "\n",
      "Prompt \n",
      " \t  What are the findings? => \n",
      "Summary \n",
      " \t  </s> in this paper, we introduce a new language representation model called the bidirectional encoder</s>\n",
      "\n",
      "\n",
      "Prompt \n",
      " \t  The main contribution of this paper are: (1) => \n",
      "Summary \n",
      " \t  </s> the main contribution of this paper are : ( 1 ) we introduce a new language representation model</s>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "abstract = \"\"\"\n",
    "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n",
    "BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n",
    "\"\"\"\n",
    "\n",
    "_ = summarize(\"Q:What is the the proposed method in the paper? A: => \")\n",
    "_ = summarize(\"findings => \" )\n",
    "_ = summarize(\"What are the findings? => \" )\n",
    "_ = summarize(\"The main contribution of this paper are: (1) => \" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa722aa51baa7ca1a14ae10c51947100c8742c9283df2adfc9442d206b591bbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
