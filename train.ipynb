{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_path = \"task1/train/eLife_train.jsonl\"\n",
    "with open(file_path, 'r') as f:\n",
    "    lst = []\n",
    "    for line in f.readlines():\n",
    "        text = json.loads(line)\n",
    "        lst.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_fast.PreTrainedTokenizerFast"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,  1121,     5,  2805,  2156,    55,  3257,  1369,    11,     5,\n",
      "          2608,    87,     5,  1035,   479,   125,    77,  3257,  5948, 23653,\n",
      "          8908,    30,  2099,  2156,  1046,  2156,  1303,     9,   744,  2156,\n",
      "             8,  3544,   976,   479,  5446,   337,  5550,    11,   744,  1162,\n",
      "            64,   464,    81,    86,   528,     7,  1022,    11,  2433,    14,\n",
      "          1303,  2199,    50,  3327,  1416,   479,  4749, 10293,     5,   191,\n",
      "          6948,     9,  3257,    64,   244,  4211,  3094,   549, 15985,     7,\n",
      "         15925,  3257,   148,    10,  1402,    86,     9,    76,    32,   956,\n",
      "          2156,    50,   549,  2210,  1980,    32,  2375,   479,  2741,   338,\n",
      "          1182,   179,  2787, 10175,  8117,    11,   744,    81,    86,    64,\n",
      "            67,   244,  4211,  3094,   549,   739,    12,  8056,  1650,    50,\n",
      "          2147,  1022,    32,  7920,     5,   191,  6948,     9,   744,   479,\n",
      "           978,  2156,  8938,  4400,  1076,   479,   311,    14,    89,    32,\n",
      "          1046,     8,  2099,  5550,    11,    61,   498,     9,    76,   144,\n",
      "          3257,  5948,   479,  8938,  4400,  1076,   479, 13773,   414,    15,\n",
      "           382,  3257,   227,  5114,     8,   336,   479,   616,  1374,  3257,\n",
      "            11,    10,    76,    58,  1609,    11,  2608,     8,  3912,    11,\n",
      "          1035,  2156,    10,  2388,   346,     9,   664,   604,   962,   148,\n",
      "          1035,   126,  4412,   528,     7,  1746,   126,    87,   148,  2608,\n",
      "           479,  5446,   337,  5550,    11,  3257,   566,   664,   408,    33,\n",
      "          2743,  9939,     8, 10175,  5550,    11,     5,  3257,     9,  2530,\n",
      "           408,     8,   664,  3362,    33,   555,  2735,   479, 41856,   566,\n",
      "           390,     8,   604,  5180,  2248,    50,  2530, 21174,   227,   719,\n",
      "             8,   902,   126,  2743,  1726,    30, 17960,     8,  1144,  6357,\n",
      "          2156,    50,  1746,   479, 41856,    11,    42,  2530,  1046,   333,\n",
      "            58,  3912,   148,     5,  1035,   377,   479, 11644,  8117,    11,\n",
      "          2530,    82,  1714,   410,    81,    86,   479,   440,  2174,  5550,\n",
      "            58,   303,    11, 10175,   744,  8117,  2156,  1135,   739,  2147,\n",
      "         21875,   420,     5,  2805,   479,    20,  1966,    30,  8938,  4400,\n",
      "          1076,   479,  3649,   285,   474,     8,  1131, 15985,    33,    57,\n",
      "          1800,    11,  4881, 10175,  3257,   566,   171,  1134,   479,   125,\n",
      "            55,   782,     7,    28,   626,     7,  1100, 10175,  5550,    11,\n",
      "          3257,   566,  2530,  3362,   479,   286,  1246,  2156,    30, 11606,\n",
      "          6626, 21911,  1162,  2156,  1976,  8383,    59,  3814,  1650,     8,\n",
      "           357, 27386,    13,  1611,   479,  8630,   806,   101,  1420,    12,\n",
      "          3743,  4358,  2110,    50,   184,  5695,     7,   244,   489,  4478,\n",
      "          7497,    82,  3665,   148,     5,  2608,   377,   189,    67,   244,\n",
      "           479,     2]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, PreTrainedTokenizerFast\n",
    "from transformers.models.bart.modeling_bart import BartForConditionalGeneration\n",
    "\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"facebook/bart-base\")\n",
    "summary = tokenizer([lst[0]['lay_summary']], return_tensors=\"pt\", padding=\"max_length\" )\n",
    "target_ids, target_mask = summary[\"input_ids\"], summary[\"attention_mask\"]\n",
    "print(target_ids)\n",
    "\n",
    "# summary = tokenizer([lst[0]['article']], return_tensors=\"pt\", padding=\"max_length\" )\n",
    "# input_ids, attention_mask = summary[\"input_ids\"], summary[\"attention_mask\"]\n",
    "\n",
    "# bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_batch_encode_plus() got an unexpected keyword argument 'text_target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_184683/1098150061.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'article'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_target\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lay_summary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtarget_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/baseline/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2304\u001b[0m                 \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2305\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2306\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2307\u001b[0m             )\n\u001b[1;32m   2308\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/baseline/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2489\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2490\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2491\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2492\u001b[0m         )\n\u001b[1;32m   2493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: _batch_encode_plus() got an unexpected keyword argument 'text_target'"
     ]
    }
   ],
   "source": [
    "summary = tokenizer([lst[0]['article']], text_target=[lst[0]['lay_summary']], return_tensors=\"pt\", padding=\"max_length\" )\n",
    "target_ids, target_mask = summary[\"input_ids\"], summary[\"attention_mask\"]\n",
    "print(target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart(input_ids=input_ids, attention_mask=attention_mask, decoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa722aa51baa7ca1a14ae10c51947100c8742c9283df2adfc9442d206b591bbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
