{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data from :  data/task1/train/eLife_train.jsonl\n",
      "The number of data:  4346\n"
     ]
    }
   ],
   "source": [
    "from utils import read_data\n",
    "from transformers import AutoModelForSeq2SeqLM, PreTrainedTokenizerFast, AutoTokenizer\n",
    "from transformers.models.bart.modeling_bart import BartForConditionalGeneration\n",
    "import torch.functional as F\n",
    "import torch\n",
    "\n",
    "# load data\n",
    "file_path = \"data/task1/train/eLife_train.jsonl\"\n",
    "articles, summaries = read_data(file_path)\n",
    "\n",
    "bart_tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large')\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n",
    "# by default encoder-attention is `block_sparse` with num_random_blocks=3, block_size=64\n",
    "# model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\") # 2.31G"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Selection by Rouge Scores\n",
    "select silient sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rouge import Rouge\n",
    "from nltk import tokenize\n",
    "rouge_pltrdy = Rouge()\n",
    "\n",
    "\n",
    "def get_rouge2recall_scores_nopad(sentences, reference, oracle_type=None):\n",
    "    # rouge_pltrdy is case sensitive\n",
    "    reference = reference.lower()\n",
    "    scores = [None for _ in range(len(sentences))]\n",
    "    count_nonzero_rouge2recall = 0\n",
    "    for i, sent in enumerate(sentences):\n",
    "        sent = sent.lower()\n",
    "        try:\n",
    "            rouge_scores = rouge_pltrdy.get_scores(sent, reference)\n",
    "            scores[i]  = rouge_scores[0]['rouge-2']['r'] # rouge2recall\n",
    "        except ValueError:\n",
    "            scores[i] = 0.0\n",
    "        except RecursionError:\n",
    "            scores[i] = 0.5 # just assign 0.5 as this sentence is simply too long\n",
    "        if scores[i] > 0.0: count_nonzero_rouge2recall += 1\n",
    "    # print('count_nonzero_rouge2recall=', count_nonzero_rouge2recall)\n",
    "    scores = np.array(scores)\n",
    "    N = len(scores)\n",
    "\n",
    "    if oracle_type == 'padlead':\n",
    "        biases = np.array([(N-i)*1e-12 for i in range(N)])\n",
    "    elif oracle_type == 'padrand':\n",
    "        biases = np.random.normal(scale=1e-10,size=(N,))\n",
    "    else: # no pad \n",
    "        return np.array(scores)\n",
    "    return np.array(scores) + biases\n",
    "\n",
    "def compress_article(article):\n",
    "    sentences = tokenize.sent_tokenize(article)\n",
    "    # print(f'There are {len(sentences)} sentences.')\n",
    "    reference = summaries[0]\n",
    "\n",
    "    ## rank by ROUGH\n",
    "    keep_idx = []\n",
    "    scores = get_rouge2recall_scores_nopad( sentences, reference, oracle_type='padrand' )\n",
    "    num_postive = sum(a > 0 for a in scores)\n",
    "    rank = np.argsort(scores)[::-1][:num_postive] # only consider positive ones\n",
    "\n",
    "    ## select high-ranked sentences\n",
    "    keep_idx = []\n",
    "    total_length = 0\n",
    "    max_abssum_len = 1024-2\n",
    "    for sent_i in rank:\n",
    "        if total_length < max_abssum_len:\n",
    "            sent = sentences[sent_i]\n",
    "            total_length += len(bart_tokenizer.encode(sent)[1:-1]) # ignore <s> and </s>\n",
    "            keep_idx.append(sent_i)\n",
    "        else:\n",
    "            break\n",
    "    assert len(keep_idx) > 0\n",
    "    ## if found nothing, selecting the top3 longest sentences\n",
    "    # if len(keep_idx) == 0:\n",
    "    #     sent_lengths = [len(tokenize.word_tokenize(ssent)) for ssent in sentences]\n",
    "    #     keep_idx = np.argsort(sent_lengths)[::-1][:3].tolist()\n",
    "    keep_idx = sorted(keep_idx) # back to original order\n",
    "    filtered_sentences = [sentences[j] for j in keep_idx]\n",
    "    filtered_input_text = \" \".join(filtered_sentences)\n",
    "    return filtered_input_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 126 sentences.\n",
      "count_nonzero_rouge2recall= 73\n"
     ]
    }
   ],
   "source": [
    "compressed_articles = []\n",
    "for i, article in enumerate(articles):\n",
    "    print(i)\n",
    "    filtered_input_text = compress_article(article)\n",
    "    compressed_articles.append(compressed_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'However , there is limited information on the timing and the relative magnitudes of maximum and minimum mortality , by local climate , age group , sex and medical cause of death . We used geo-coded mortality data and wavelets to analyse the seasonality of mortality by age group and sex from 1980 to 2016 in the USA and its subnational climatic regions . In adolescents and young adults , especially in males , death rates peaked in June/July and were lowest in December/January , driven by injury deaths . It is well-established that death rates vary throughout the year , and in temperate climates there tend to be more deaths in winter than in summer ( Campbell , 2017; Fowler et al . In a large country like the USA , which possesses distinct climate regions , the seasonality of mortality may vary geographically , due to geographical variations in mortality , localized weather patterns , and regional differences in adaptation measures such as heating , air conditioning and healthcare ( Davis et al . Although mortality seasonality is well-established , there is limited information on how seasonality , including the timing of minimum and maximum mortality , varies by local climate and how these features have changed over time , especially in relation to age group , sex and medical cause of death ( Rau , 2004; Rau et al . In this paper , we comprehensively characterize the spatial and temporal patterns of all-cause and cause-specific mortality seasonality in the USA by sex and age group , through the application of wavelet analytical techniques , to over three decades of national mortality data . The strengths of our study are its innovative methods of characterizing seasonality of mortality dynamically over space and time , by age group and cause of death; using wavelet and centre of gravity analyses; using ERA-Interim data output to compare the association between seasonality of death rates and regional temperature . We used wavelet and centre of gravity analyses , which allowed systematically identifying and characterizing seasonality of total and cause-specific mortality in the USA , and examining how seasonality has changed over time . We identified distinct seasonal patterns in relation to age and sex , including higher all-cause summer mortality in young men ( Feinstein , 2002; Rau et al . Prior studies have noted seasonality of mortality for all-cause mortality and for specific causes of death in the USA ( Feinstein , 2002; Kalkstein , 2013; Rau , 2004; Rau et al . A study of 36 cities in the USA , aggregated across age groups and over time , also found that excess mortality was not associated with seasonal temperature range ( Kinney et al . If the observed absence of association between the magnitude of mortality seasonality and seasonal temperature difference across the climate regions also persists over time , the changes in temperature as a result of global climate change are unlikely to affect the winter-summer mortality difference . The cause-specific analysis showed that the substantial decline in seasonal mortality differences in adolescents and young adults was related to the diminishing seasonality of ( unintentional ) injuries , especially from road traffic crashes , which are more likely to occur in the summer months ( Liu et al . The weakening of seasonality in boys under five years of age was related to two phenomena: first , the seasonality of death from cardiorespiratory diseases declined , and second , the proportion of deaths from perinatal conditions , which exhibit limited seasonality ( Figure 9—figure supplement 2 and Figure 10—figure supplement 3 ) , increased ( MacDorman and Gregory , 2015 ) . Age , sex , state of residence , month of death , and underlying cause of death were available for each record . For analysis of seasonality by cause of death , we mapped each ICD-9 and ICD-10 codes to four main disease categories ( Table 1 ) and to a number of subcategories which are presented in the Supplementary Note . 2% of all deaths in the USA , respectively , in 1980 , and 40 . , 2008 ) , is equivalent to using a moving window on the death rate time series and analysing periodicity in each window using a short-form Fourier transform , hence generating a dynamic spectral analysis , which allows measuring dynamic seasonal patterns , in which the periodicity of death rates may disappear , emerge , or change over time . Using a technique called circular statistics , a mean ( θ- ) of the angles ( θ1 , θ2 , θ3… , θn , ) representing the deaths ( with n the total number of deaths in an age-sex group for a particular cause of death ) is found using the relation below:θ-=arg∑j=1nexp\\u2061 ( iθj ) , where arg denotes the complex number argument and θj denotes the month of death in angular form for a particular death j . For each age-sex group and cause of death , and for each year , we calculated the percent difference in death rates between the maximum and minimum mortality months . Our method of analysing seasonal differences avoids assuming that any specific month or group of months represent highest and lowest number of deaths for a particular cause of death , which is the approach taken by the traditional measure of Excess Winter Deaths .'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open(out_path, \"w\") as f:\n",
    "#     f.write(filtered_input_text)\n",
    "# print(\"write:\", out_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"facebook/bart-base\") # no <pad> token\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "# text encoding\n",
    "# with tokenizer.as_target_tokenizer(): # same behaviour with/w.o context manager\n",
    "summary = tokenizer([lst[0]['lay_summary']], return_tensors=\"pt\", padding=\"longest\" ) \n",
    "target_ids, target_mask = summary[\"input_ids\"], summary[\"attention_mask\"] # (bsz, target_seq_len)\n",
    "\n",
    "article = tokenizer([lst[0]['article']], return_tensors=\"pt\", padding=\"max_length\", truncation=True )\n",
    "input_ids, attention_mask = article[\"input_ids\"], article[\"attention_mask\"] # (bsz, 1024)\n",
    "\n",
    "\n",
    "\n",
    "def sequence_cross_entropy_with_logits(logits, shifted_target_ids, shifted_target_mask):\n",
    "    # flatten\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    targets_flatten = shifted_target_ids.view(-1)\n",
    "    return F.cross_entropy(logits_flat, targets_flatten, shifted_target_mask)\n",
    "\n",
    "\n",
    "\n",
    "bart_output = bart(\n",
    "    input_ids=input_ids, \n",
    "    attention_mask=attention_mask, \n",
    "    decoder_input_ids=target_ids[:, :-1].contiguous(),\n",
    "    decoder_attention_mask=target_mask[:, :-1].contiguous(),\n",
    "    use_cache=False,\n",
    "    return_dict=True \n",
    ")\n",
    "logits = bart_output.logits # (bsz, target_seq_len-1, vocab_size), '-1' for the last position\n",
    "shifted_target_ids =  target_ids[:, 1:].type(torch.LongTensor).contiguous() # (bsz, target_seq_len-1, vocab_size), '-1' for the first position\n",
    "shifted_target_mask = target_mask[:, 1:].type(torch.LongTensor).contiguous()\n",
    "loss = sequence_cross_entropy_with_logits(logits, shifted_target_ids, shifted_target_mask, shifted_target_mask)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa722aa51baa7ca1a14ae10c51947100c8742c9283df2adfc9442d206b591bbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
