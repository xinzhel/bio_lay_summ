{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data from :  data/task1/train/eLife_train.jsonl\n",
      "The number of data:  4346\n"
     ]
    }
   ],
   "source": [
    "from utils import read_data, read_instances\n",
    "from transformers import AutoModelForSeq2SeqLM, PreTrainedTokenizerFast, AutoTokenizer\n",
    "from transformers.models.bart.modeling_bart import BartForConditionalGeneration\n",
    "import torch.functional as F\n",
    "import torch\n",
    "\n",
    "# load data\n",
    "file_path = \"data/task1/train/eLife_train.jsonl\"\n",
    "instances = read_instances(file_path)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n",
    "# by default encoder-attention is `block_sparse` with num_random_blocks=3, block_size=64\n",
    "# model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\") # 2.31G"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Selection by Rouge Scores\n",
    "select silient sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rouge import Rouge\n",
    "from nltk import tokenize\n",
    "rouge_pltrdy = Rouge()\n",
    "\n",
    "\n",
    "def get_rouge2recall_scores_nopad(sentences, reference, oracle_type=None):\n",
    "    # rouge_pltrdy is case sensitive\n",
    "    reference = reference.lower()\n",
    "    scores = [None for _ in range(len(sentences))]\n",
    "    count_nonzero_rouge2recall = 0\n",
    "    for i, sent in enumerate(sentences):\n",
    "        sent = sent.lower()\n",
    "        try:\n",
    "            rouge_scores = rouge_pltrdy.get_scores(sent, reference)\n",
    "            scores[i]  = rouge_scores[0]['rouge-2']['r'] # rouge2recall\n",
    "        except ValueError:\n",
    "            scores[i] = 0.0\n",
    "        except RecursionError:\n",
    "            scores[i] = 0.5 # just assign 0.5 as this sentence is simply too long\n",
    "        if scores[i] > 0.0: count_nonzero_rouge2recall += 1\n",
    "    # print('count_nonzero_rouge2recall=', count_nonzero_rouge2recall)\n",
    "    scores = np.array(scores)\n",
    "    N = len(scores)\n",
    "\n",
    "    if oracle_type == 'padlead':\n",
    "        biases = np.array([(N-i)*1e-12 for i in range(N)])\n",
    "    elif oracle_type == 'padrand':\n",
    "        biases = np.random.normal(scale=1e-10,size=(N,))\n",
    "    else: # no pad \n",
    "        return np.array(scores)\n",
    "    return np.array(scores) + biases\n",
    "\n",
    "def compress_article(article):\n",
    "    sentences = tokenize.sent_tokenize(article)\n",
    "    # print(f'There are {len(sentences)} sentences.')\n",
    "    reference = summaries[0]\n",
    "\n",
    "    ## rank by ROUGH\n",
    "    keep_idx = []\n",
    "    scores = get_rouge2recall_scores_nopad( sentences, reference, oracle_type='padrand' )\n",
    "    num_postive = sum(a > 0 for a in scores)\n",
    "    rank = np.argsort(scores)[::-1][:num_postive] # only consider positive ones\n",
    "\n",
    "    ## select high-ranked sentences\n",
    "    keep_idx = []\n",
    "    total_length = 0\n",
    "    max_abssum_len = 1024-2\n",
    "    for sent_i in rank:\n",
    "        if total_length < max_abssum_len:\n",
    "            sent = sentences[sent_i]\n",
    "            total_length += len(bart_tokenizer.encode(sent)[1:-1]) # ignore <s> and </s>\n",
    "            keep_idx.append(sent_i)\n",
    "        else:\n",
    "            break\n",
    "    assert len(keep_idx) > 0\n",
    "    ## if found nothing, selecting the top3 longest sentences\n",
    "    # if len(keep_idx) == 0:\n",
    "    #     sent_lengths = [len(tokenize.word_tokenize(ssent)) for ssent in sentences]\n",
    "    #     keep_idx = np.argsort(sent_lengths)[::-1][:3].tolist()\n",
    "    keep_idx = sorted(keep_idx) # back to original order\n",
    "    filtered_sentences = [sentences[j] for j in keep_idx]\n",
    "    filtered_input_text = \" \".join(filtered_sentences)\n",
    "    return filtered_input_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 126 sentences.\n",
      "count_nonzero_rouge2recall= 73\n"
     ]
    }
   ],
   "source": [
    "compressed_articles = []\n",
    "for i, article in enumerate(articles):\n",
    "    print(i)\n",
    "    filtered_input_text = compress_article(article)\n",
    "    compressed_articles.append(compressed_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'However , there is limited information on the timing and the relative magnitudes of maximum and minimum mortality , by local climate , age group , sex and medical cause of death . We used geo-coded mortality data and wavelets to analyse the seasonality of mortality by age group and sex from 1980 to 2016 in the USA and its subnational climatic regions . In adolescents and young adults , especially in males , death rates peaked in June/July and were lowest in December/January , driven by injury deaths . It is well-established that death rates vary throughout the year , and in temperate climates there tend to be more deaths in winter than in summer ( Campbell , 2017; Fowler et al . In a large country like the USA , which possesses distinct climate regions , the seasonality of mortality may vary geographically , due to geographical variations in mortality , localized weather patterns , and regional differences in adaptation measures such as heating , air conditioning and healthcare ( Davis et al . Although mortality seasonality is well-established , there is limited information on how seasonality , including the timing of minimum and maximum mortality , varies by local climate and how these features have changed over time , especially in relation to age group , sex and medical cause of death ( Rau , 2004; Rau et al . In this paper , we comprehensively characterize the spatial and temporal patterns of all-cause and cause-specific mortality seasonality in the USA by sex and age group , through the application of wavelet analytical techniques , to over three decades of national mortality data . The strengths of our study are its innovative methods of characterizing seasonality of mortality dynamically over space and time , by age group and cause of death; using wavelet and centre of gravity analyses; using ERA-Interim data output to compare the association between seasonality of death rates and regional temperature . We used wavelet and centre of gravity analyses , which allowed systematically identifying and characterizing seasonality of total and cause-specific mortality in the USA , and examining how seasonality has changed over time . We identified distinct seasonal patterns in relation to age and sex , including higher all-cause summer mortality in young men ( Feinstein , 2002; Rau et al . Prior studies have noted seasonality of mortality for all-cause mortality and for specific causes of death in the USA ( Feinstein , 2002; Kalkstein , 2013; Rau , 2004; Rau et al . A study of 36 cities in the USA , aggregated across age groups and over time , also found that excess mortality was not associated with seasonal temperature range ( Kinney et al . If the observed absence of association between the magnitude of mortality seasonality and seasonal temperature difference across the climate regions also persists over time , the changes in temperature as a result of global climate change are unlikely to affect the winter-summer mortality difference . The cause-specific analysis showed that the substantial decline in seasonal mortality differences in adolescents and young adults was related to the diminishing seasonality of ( unintentional ) injuries , especially from road traffic crashes , which are more likely to occur in the summer months ( Liu et al . The weakening of seasonality in boys under five years of age was related to two phenomena: first , the seasonality of death from cardiorespiratory diseases declined , and second , the proportion of deaths from perinatal conditions , which exhibit limited seasonality ( Figure 9—figure supplement 2 and Figure 10—figure supplement 3 ) , increased ( MacDorman and Gregory , 2015 ) . Age , sex , state of residence , month of death , and underlying cause of death were available for each record . For analysis of seasonality by cause of death , we mapped each ICD-9 and ICD-10 codes to four main disease categories ( Table 1 ) and to a number of subcategories which are presented in the Supplementary Note . 2% of all deaths in the USA , respectively , in 1980 , and 40 . , 2008 ) , is equivalent to using a moving window on the death rate time series and analysing periodicity in each window using a short-form Fourier transform , hence generating a dynamic spectral analysis , which allows measuring dynamic seasonal patterns , in which the periodicity of death rates may disappear , emerge , or change over time . Using a technique called circular statistics , a mean ( θ- ) of the angles ( θ1 , θ2 , θ3… , θn , ) representing the deaths ( with n the total number of deaths in an age-sex group for a particular cause of death ) is found using the relation below:θ-=arg∑j=1nexp\\u2061 ( iθj ) , where arg denotes the complex number argument and θj denotes the month of death in angular form for a particular death j . For each age-sex group and cause of death , and for each year , we calculated the percent difference in death rates between the maximum and minimum mortality months . Our method of analysing seasonal differences avoids assuming that any specific month or group of months represent highest and lowest number of deaths for a particular cause of death , which is the approach taken by the traditional measure of Excess Winter Deaths .'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open(out_path, \"w\") as f:\n",
    "#     f.write(filtered_input_text)\n",
    "# print(\"write:\", out_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11893"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained(\"facebook/bart-base\") # no <pad> token\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "# tokenizer(summaries[:2], padding=True, return_tensors='pt')['input_ids']\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# create a dataset class for data loader\n",
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, instances):\n",
    "        self.instances = instances\n",
    "        # sort the batch in the descending order of the number of tokens in the article\n",
    "        instances.sort(key=lambda x: len(x['article']), reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.instances[idx]['article'], self.instances[idx]['lay_summary']\n",
    "\n",
    "# create a collate function for data loader to convert a list of texts in to a batched input tensor\n",
    "def collate_fn(batch):\n",
    "    articles, summaries = zip(*batch)\n",
    "    # tokenize the articles and summaries\n",
    "    encodings = tokenizer(articles, summaries, padding=True, truncation=True, return_tensors='pt')\n",
    "    return encodings\n",
    "\n",
    "# create a data loader\n",
    "dataset = SummarizationDataset(instances)\n",
    "dataloader = DataLoader(dataset, batch_size=8, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 20:41:41.242810: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-08 20:41:41.563668: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-08 20:41:42.544270: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-08 20:41:42.544657: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-08 20:41:42.544667: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "  0%|          | 0/544 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4555/2408215305.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mbart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# iterate over the batches of the training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# batch = {k: v.to(device) for k, v in batch.items()}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/baseline/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/baseline/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/baseline/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/baseline/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/baseline/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4555/769550518.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'article'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# create a collate function for data loader to convert a list of texts in to a batched input tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'summary'"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW\n",
    "\n",
    "# set the device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "bart.to(device)\n",
    "\n",
    "# set the optimizer\n",
    "optimizer = AdamW(bart.parameters(), lr=1e-5)\n",
    "\n",
    "# set the number of epochs\n",
    "epochs = 10\n",
    "\n",
    "# iterate over epochs\n",
    "for epoch in range(epochs):\n",
    "    # set the model to training mode\n",
    "    bart.train()\n",
    "    # iterate over the batches of the training set\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch\n",
    "        # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        # optimizer.zero_grad()\n",
    "        # outputs = bart(**batch)\n",
    "        # loss = outputs.loss\n",
    "        # loss.backward()\n",
    "        # optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# text encoding\n",
    "# with tokenizer.as_target_tokenizer(): # same behaviour with/w.o context manager\n",
    "encodings = tokenizer(dataset[0][0], dataset[0][1], padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "\n",
    "# summary = tokenizer([summaries[0]], return_tensors=\"pt\", padding=\"longest\" ) \n",
    "# target_ids, target_mask = summary[\"input_ids\"], summary[\"attention_mask\"] # (bsz, target_seq_len)\n",
    "\n",
    "# article = tokenizer([articles[0]], return_tensors=\"pt\", padding=\"max_length\", truncation=True )\n",
    "# input_ids, attention_mask = article[\"input_ids\"], article[\"attention_mask\"] # (bsz, 1024)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[511, 512, 1023]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst2 = encodings['input_ids'][0].tolist()\n",
    "# find all the indices of 2 in lst2\n",
    "indices = [i for i, x in enumerate(lst2) if x == 2]\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 28324,   119,  ..., 10543, 16570,     2]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(encodings)\n",
    "# check attributes of encodings\n",
    "encodings['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sequence_cross_entropy_with_logits(logits, shifted_target_ids, shifted_target_mask):\n",
    "    # flatten\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    targets_flatten = shifted_target_ids.view(-1)\n",
    "    return F.cross_entropy(logits_flat, targets_flatten, shifted_target_mask)\n",
    "\n",
    "\n",
    "\n",
    "bart_output = bart(\n",
    "    input_ids=input_ids, \n",
    "    attention_mask=attention_mask, \n",
    "    decoder_input_ids=target_ids[:, :-1].contiguous(),\n",
    "    decoder_attention_mask=target_mask[:, :-1].contiguous(),\n",
    "    use_cache=False,\n",
    "    return_dict=True \n",
    ")\n",
    "logits = bart_output.logits # (bsz, target_seq_len-1, vocab_size), '-1' for the last position\n",
    "shifted_target_ids =  target_ids[:, 1:].type(torch.LongTensor).contiguous() # (bsz, target_seq_len-1, vocab_size), '-1' for the first position\n",
    "shifted_target_mask = target_mask[:, 1:].type(torch.LongTensor).contiguous()\n",
    "loss = sequence_cross_entropy_with_logits(logits, shifted_target_ids, shifted_target_mask, shifted_target_mask)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa722aa51baa7ca1a14ae10c51947100c8742c9283df2adfc9442d206b591bbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
